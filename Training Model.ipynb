{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import spacy\n",
    "\n",
    "import oauth2client.file, oauth2client.tools\n",
    "from oauth2client import client\n",
    "import gspread\n",
    "\n",
    "client_id = '403167867560-kgeqncsauc7lht11sfkjk1klup9v06oa.apps.googleusercontent.com'\n",
    "client_secret = 'vgoDh7To1xrDHYqb3wc5Lssr'\n",
    "\n",
    "flow = client.OAuth2WebServerFlow(client_id, client_secret, 'https://spreadsheets.google.com/feeds')\n",
    "storage = oauth2client.file.Storage('credentials.dat')\n",
    "credentials = storage.get()\n",
    "if credentials is None or credentials.invalid:\n",
    "    import argparse\n",
    "    flags = argparse.ArgumentParser(parents=[oauth2client.tools.argparser]).parse_args([])\n",
    "    credentials = oauth2client.tools.run_flow(flow, storage, flags)\n",
    "\n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "from df2gspread import df2gspread as d2g\n",
    "from df2gspread import gspread2df as g2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EntityMatcher(object):\n",
    "    name = \"entity_matcher\"\n",
    "\n",
    "    def __init__(self, nlp, terms, label):\n",
    "        patterns = [nlp.make_doc(text) for text in terms]\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add(label, None, *patterns)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        matches = self.matcher(doc)\n",
    "        for match_id, start, end in matches:\n",
    "            span = Span(doc, start, end, label=match_id)\n",
    "            doc.ents = list(doc.ents) + [span]\n",
    "        return doc    \n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "entity_matcher = EntityMatcher(nlp, terms, \"SOFT SKILL\")\n",
    "\n",
    "nlp.add_pipe(entity_matcher, after=\"ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E103] Trying to set conflicting doc.ents: '(78, 81, 'SOFT SKILL')' and '(80, 81, 'SOFT SKILL')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-d0d65d2c7fd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spacy_env/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-5ff60aeaaab6>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E103] Trying to set conflicting doc.ents: '(78, 81, 'SOFT SKILL')' and '(80, 81, 'SOFT SKILL')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap."
     ]
    }
   ],
   "source": [
    "nlp(df.loc[0,'abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E103] Trying to set conflicting doc.ents: '(78, 81, 'SOFT SKILL')' and '(80, 81, 'SOFT SKILL')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-25c76906988d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spacy_env/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-5ff60aeaaab6>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E103] Trying to set conflicting doc.ents: '(78, 81, 'SOFT SKILL')' and '(80, 81, 'SOFT SKILL')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap."
     ]
    }
   ],
   "source": [
    "doc = nlp(df.loc[0,'abstract'])\n",
    "matcher(doc)\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82438d3471d1451184e6d895debd2ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87baef8eb0046d6987b4e36b67948c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses {'ner': 6384.86925319556}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a87a6296d1c4ae490c72012d0120962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses {'ner': 2880.2105161005484}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240d2f92173742259b9c37a818fd7e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses {'ner': 2239.718901356168}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78aee515466f465f84db343647c3ce05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses {'ner': 2051.1399134368653}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04077758a1ba43658be0bff1dac92c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses {'ner': 1886.7062138716801}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66448c20b634ed8b227d751bb55da7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Losses {'ner': 1566.179482715063}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c78b73995b14eea9dca155102f03481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "nlp = spacy.load(\"en_core_web_sm\",disable = ['ner'])  # create blank Language class\n",
    "# Add entity recognizer to model if it's not in the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "\n",
    "\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Only run nlp.make_doc to speed things up\n",
    "def add_phraseMatcher_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label = \"SOFT SKILL\")\n",
    "    try: \n",
    "        doc.ents += (entity,)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", add_phraseMatcher_ent, *patterns)\n",
    "nlp.to_disk(\"soft_skills\")\n",
    "\n",
    "ner.add_label('SOFT SKILL')\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "move_names = list(ner.move_names)\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    for itn in tqdm(range(25)):\n",
    "        random.shuffle(TRAIN_)\n",
    "        batches = minibatch(TRAIN_, size=sizes)\n",
    "        losses = {}\n",
    "        for batch in tqdm(batches):\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            \n",
    "        nlp.to_disk(\"soft_skills\")\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'problem solving'\n",
      "SOFT SKILL problem solving\n"
     ]
    }
   ],
   "source": [
    "test_text = \"problem solving\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROPN', 'PROPN', 'AUX', 'DET', 'NOUN', 'ADP', 'PROPN']"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.pos_ for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the Lexicon from the web\n",
    "lexicon = pd.read_excel('softSkillsLexicon.xlsx')\n",
    "lexicon['soft_skill'] = lexicon['soft_skill'].apply(lambda x: x[1:] if x.startswith(' ') else x)\n",
    "lexicon['soft_skill'] = lexicon['soft_skill'].apply(lambda x: x[:-1] if x.endswith(' ') else x)\n",
    "lexicon['len'] = lexicon.apply(lambda x: len(x['soft_skill']), axis = 1)\n",
    "lexicon.sort_values(by = 'len')[:30]\n",
    "lexicon = lexicon[~lexicon['soft_skill'].isin(['try','design','strive','decide','choose','persist','bargain','attempt','endeavour','innovate','continue','schedule'])]\n",
    "\n",
    "terms = lexicon['soft_skill'].unique().tolist()\n",
    "terms.append('problem-solving')\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Only run nlp.make_doc to speed things up\n",
    "def add_phraseMatcher_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label = \"SOFT SKILL\")\n",
    "    try: \n",
    "        doc.ents += (entity,)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", add_phraseMatcher_ent, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('soft_skill_rule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('terms.txt', 'w') as f:\n",
    "    for item in terms:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open('terms.txt', 'r') as f:\n",
    "    terms = f.readlines()\n",
    "terms = [t.replace('\\n','') for t in terms if t != '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3766102292120407359, 78, 81),\n",
       " (3766102292120407359, 80, 81),\n",
       " (3766102292120407359, 85, 86)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher(nlp(df.loc[0,'abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = g2d.download('1IbmUHYiQ803h2nCDfWI5EQBD6K8UUwdz7gETMdRvtrE',wks_name='Abstracts',credentials = credentials, col_names=True)\n",
    "df = df[['docId','abstract']]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7272074a709461586ecbb4671543f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10519.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 93/10519 [00:20<08:02, 21.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def as_doc(x):\n",
    "    \"\"\"Transform a cell string of a dataframe in a spacy_doc\"\"\"\n",
    "    \n",
    "    doc = nlp(x)\n",
    "    matcher(doc)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "#if os.path.exists('./data/interim/cv_database_spacy.pickle'):\n",
    "#    df = pd.read_pickle('./data/interim/cv_database_spacy.pickle')\n",
    "#    print('File exists.')\n",
    "#df = pd.read_excel('./data/interim/cv_database.xlsx')\n",
    "df['abstract_doc'] = df.progress_apply(lambda x: as_doc(x['abstract']),axis = 1)\n",
    "#print('Pickling...')\n",
    "#df.to_pickle('corpus.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01f4928613646e9aa12e07c2c346130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10519.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _,row in tqdm(df.iterrows(), total = len(df)):\n",
    "    doc = row['abstract_doc']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(sense of responsibility, 'SOFT SKILL'), (confidence, 'SOFT SKILL')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ent,ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cda4d4276224a9b90c5a5365c25ee93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10519.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN = []\n",
    "\n",
    "for _,row in tqdm(df.iterrows(), total = len(df)):\n",
    "    doc = row['abstract_doc']\n",
    "    sents = [s.as_doc() for s in doc.sents]\n",
    "    \n",
    "    for sent in sents:\n",
    "        \n",
    "        try:\n",
    "            doc = sent.to_json()\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        text = doc['text']\n",
    "        ents = {}\n",
    "        ents['entities'] = []\n",
    "        \n",
    "        if 'ents' not in doc.keys():\n",
    "            continue\n",
    "        \n",
    "        for ent in doc['ents']:\n",
    "            ents['entities'].append((ent['start'],ent['end'],ent['label']))\n",
    "        TRAIN.append((text,ents))\n",
    "#TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080cb38d93854c40b9a5e7305fb61016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12597.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_ = TRAIN\n",
    "\n",
    "clues = ['able to', 'able in','ability to','ability in','capability to','capable of','know-how of','know-how in','level of','knowledge of','experience in','experience of']\n",
    "train_terms = []\n",
    "for term in terms:\n",
    "    train_terms.append(term)\n",
    "    for clue in clues:\n",
    "        train_terms.append(clue + ' ' + term)\n",
    "        \n",
    "for skill in tqdm(train_terms, total = len(train_terms)):\n",
    "    text = skill\n",
    "    ents = {}\n",
    "    ents['entities'] = [(0,len(skill),'SOFT SKILL')]\n",
    "    TRAIN_.append((text,ents))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "spacy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
